<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Why can&#39;t TorToiSe be fine-tuned? - 第一桶鲸网站</title><meta name="Description" content="第一桶鲸"><meta property="og:title" content="Why can&#39;t TorToiSe be fine-tuned?" />
<meta property="og:description" content="TorToiSe &#x1f422; is an open-source Text-To-Speech (TTS) neural network that creates fairly authentic &amp; realistic voices. Checkpoints for local inference have been available since April last year, but its users are seemingly unable to fine-tune the model with additional voice data.
Why is this the case, and how could it be fixed?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shentongguangda.github.io/survey/tortoise-fine-tuning/" /><meta property="og:image" content="https://shentongguangda.github.io/logo.jpg"/><meta property="article:section" content="survey" />
<meta property="article:published_time" content="2023-02-11T08:13:30+08:00" />
<meta property="article:modified_time" content="2023-02-11T08:13:30+08:00" /><meta property="og:site_name" content="第一桶鲸" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://shentongguangda.github.io/logo.jpg"/>

<meta name="twitter:title" content="Why can&#39;t TorToiSe be fine-tuned?"/>
<meta name="twitter:description" content="TorToiSe &#x1f422; is an open-source Text-To-Speech (TTS) neural network that creates fairly authentic &amp; realistic voices. Checkpoints for local inference have been available since April last year, but its users are seemingly unable to fine-tune the model with additional voice data.
Why is this the case, and how could it be fixed?"/>
<meta name="application-name" content="第一桶鲸">
<meta name="apple-mobile-web-app-title" content="第一桶鲸"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://shentongguangda.github.io/survey/tortoise-fine-tuning/" /><link rel="prev" href="https://shentongguangda.github.io/survey/nus-mistake/" /><link rel="next" href="https://shentongguangda.github.io/survey/tortoise-fine-tuned/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Why can't TorToiSe be fine-tuned?",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/shentongguangda.github.io\/survey\/tortoise-fine-tuning\/"
        },"genre": "survey","keywords": "machine learning, TTS","wordcount":  1379 ,
        "url": "https:\/\/shentongguangda.github.io\/survey\/tortoise-fine-tuning\/","datePublished": "2023-02-11T08:13:30+08:00","dateModified": "2023-02-11T08:13:30+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "152334H"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="第一桶鲸网站">第一桶鲸</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/" title="苹果试玩"> 苹果试玩 </a><a class="menu-item" href="/survey/" title="调查网站"> 调查网站 </a><a class="menu-item" href="/trade/" title="交易所"> 交易所 </a><a class="menu-item" href="/tags/" title="标签"> 标签 </a><a class="menu-item" href="/categories/" title="分类"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="第一桶鲸网站">第一桶鲸</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="苹果试玩">苹果试玩</a><a class="menu-item" href="/survey/" title="调查网站">调查网站</a><a class="menu-item" href="/trade/" title="交易所">交易所</a><a class="menu-item" href="/tags/" title="标签">标签</a><a class="menu-item" href="/categories/" title="分类">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="page single special"><h1 class="single-title animate__animated animate__pulse animate__faster">Why can&#39;t TorToiSe be fine-tuned?</h1><h2 class="single-subtitle">A surprisingly cunning move</h2><div class="content" id="content"><p><a href="http://nonint.com/static/tortoise_v2_examples.html" target="_blank" rel="noopener noreffer ">TorToiSe &#x1f422;</a> is an open-source Text-To-Speech (TTS) neural network that <abbr tabindex="0" title="It is also notable for its alleged similiarity to 11labs's TTS stack." style="color:#bbb;text-decoration-style: dotted;text-decoration-line:underline">creates fairly authentic &amp; realistic voices</abbr>. Checkpoints for local inference have been available since <a href="https://huggingface.co/jbetker/tortoise-tts-v2/tree/main/.models" target="_blank" rel="noopener noreffer ">April last year</a>, but its users are seemingly unable to fine-tune the model with additional voice data.</p>
<p>Why is this the case, and how could it be fixed?</p>
<h2 id="preamble">Preamble</h2>
<p>First, let&rsquo;s understand the situation.</p>
<p>The code, <a href="https://nonint.com/2022/04/25/tortoise-architectural-design-doc/" target="_blank" rel="noopener noreffer ">design</a>, and training details for TorToiSe are well-documented. The <a href="https://github.com/neonbjb/tortoise-tts" target="_blank" rel="noopener noreffer ">main repo</a> contains all of the code needed to download and run the TTS models (which are hosted on <a href="https://huggingface.co/jbetker/tortoise-tts-v2" target="_blank" rel="noopener noreffer ">huggingface</a>). The code for training TorToiSe from scratch is covered under the <a href="https://github.com/neonbjb/DL-Art-School/tree/master/codes/models/audio/tts" target="_blank" rel="noopener noreffer ">DL-Art-School</a> project, and the overall process is well-documented in their <a href="https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA/edit" target="_blank" rel="noopener noreffer ">draft paper</a>.</p>
<p>With all of this available, it would be incredibly surprising if no one had successfully fine-tuned the model for their own purposes. As it turns out, <a href="https://twitter.com/lexman_ai" target="_blank" rel="noopener noreffer ">someone</a> has:</p>
<blockquote>
<p>What I ended up doing instead was fine-tuning the TorToiSe models on several hundred hours of clips from the Lex Fridman podcast. I picked Lex&rsquo;s podcast because it&rsquo;s one of my favourites and he frequently talks about the concepts I want to explore with this project. His podcast was initially called &ldquo;the Artificial Intelligence Podcast&rdquo; which I also find very fitting.</p>
<p>I won&rsquo;t go into the details of how I fine-tuned the model since there are some concerns with this being used maliciously. If you&rsquo;re interested in fine-tuning your own model, to save you some time <strong>I will say that a critical component needed to do it has since been removed from the public release. So you won&rsquo;t be able to do it without re-training some of the models needed from scratch.</strong></p>
<p>&ndash; <a href="https://lexman.rocks/about" target="_blank" rel="noopener noreffer ">@lexman_ai</a></p>
</blockquote>
<p>A few searches on the GitHub repo later, and the &lsquo;critical component&rsquo; is revealed:</p>
<blockquote>
<p>This model is extremely potent when fine-tuned. It can be easily used to make deep-fake voiceovers of people. I&rsquo;m sure you&rsquo;ve heard the Joe Rogan and Steve Jobs podcast - that was Tortoise. <strong>Withholding the VQVAE which is required to perform that fine-tuning is my way of de-weaponizing it</strong>, to an extent.</p>
<p>&ndash; <a href="https://github.com/neonbjb/tortoise-tts/issues/177#issuecomment-1287555902" target="_blank" rel="noopener noreffer ">@neonbjb</a></p>
</blockquote>
<h2 id="premise">Premise</h2>
<p>The &lsquo;critical component&rsquo; that has not been released is the <a href="https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA/edit#heading=h.121azdpfv4uv" target="_blank" rel="noopener noreffer "><strong>&ldquo;VQVAE&rdquo;</strong></a> (or discrete-<a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener noreffer ">VAE</a>), a model which (in this case) can encode/decode spectrograms into/from a list of integers (&ldquo;MEL tokens&rdquo;) in the range <code>[0,8192)</code>.</p>
<p>The VQVAE is <strong>necessary for training</strong>, but also <strong>irrelevant for inference</strong>.</p>
<p>During training, the model is used to convert training data (the spectrograms of raw audio files) to discrete tokens that can be used by the GPT model (as predicted outputs) and the CLVP model (as inputs):</p>
<div class="mermaid" id="id-1"></div>
<p>During inference, the MEL tokens are simply generated by the GPT model:</p>
<div class="mermaid" id="id-2"></div>
<p>If none of that makes sense to you, well shit, I tried. I recommend reading the <a href="https://nonint.com/2022/04/25/tortoise-architectural-design-doc/" target="_blank" rel="noopener noreffer ">architectural design doc</a> and the tortoise source code, especially <a href="https://github.com/152334H/tortoise-tts-fast/blob/main/tortoise/api.py" target="_blank" rel="noopener noreffer "><code>api.py</code></a> and <a href="https://github.com/152334H/tortoise-tts-fast/blob/main/tortoise/models/autoregressive.py" target="_blank" rel="noopener noreffer "><code>models/autoregressive.py</code></a>.</p>
<hr>
<p>So, you need the VQVAE for fine-tuning. Okay, the code for it is <a href="https://github.com/neonbjb/DL-Art-School/blob/master/codes/models/audio/tts/lucidrains_dvae.py" target="_blank" rel="noopener noreffer ">here</a>, the training hyperparameters are written <a href="https://github.com/neonbjb/tortoise-tts/issues/92#issuecomment-1149023261" target="_blank" rel="noopener noreffer ">here</a>, and the dataset is&hellip;</p>
<blockquote>
<p>I independently built an extended TTS dataset composed of audiobooks and podcasts scraped from the web. This data was split on 500ms silences, and any audio clip between 5-20 seconds was kept. I then fed the resulting clips through a pipeline of classifiers that I trained which remove any audio with background noise, music, poor quality (such as phone calls), multiple voices speaking at once and reverb. Due to disk space limitations, I was forced to limit the amount of scraping. The end result was 49,000 hours of cleaned audio clips.</p>
<p>I transcribed this dataset using a wav2vec2-large model. I personally fine-tuned this model to predict punctuation, as quotation marks, commas and exclamation marks are important for the purposes of generating speech but are not generally included in the training of speech recognition models. Fine-tuning was performed on LibriTTS and HiFiTTS and the pretrained model weights and transcription scripts can be found here.</p>
<p>&ndash; <a href="https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA/edit#heading=h.wh7r70k3i1jl" target="_blank" rel="noopener noreffer ">Appendix I - Extended Dataset Collection</a></p>
</blockquote>
<p>&hellip;absent, but probably not too difficult to emulate with a combination of <a href="https://github.com/OpenAI/whisper" target="_blank" rel="noopener noreffer ">Whisper</a> on <a href="https://github.com/facebookresearch/libri-light" target="_blank" rel="noopener noreffer ">LibriLight</a>, plus some additional scraping of YouTube.</p>
<p>Is that it?</p>
<h2 id="problem">Problem</h2>
<p>Unfortunately, this is not the end. Quoting from <a href="https://github.com/neonbjb/tortoise-tts/issues/33#issuecomment-1120576446" target="_blank" rel="noopener noreffer ">yet another issue</a>:</p>
<blockquote>
<p>If you are interested in the VQVAE model itself, I would be glad to divulge training details for it so you can build your own. <strong>It will not be compatible with the Tortoise codes, though</strong> (unless you are really lucky. :) )</p>
</blockquote>
<p>To understand what this means, it is necessary to describe how the VQVAE works.</p>
<figure><img src="R9VMWD6.0.png"/>
</figure>

<p>If you&rsquo;d like a more accurate description with more detail, I recommend reading <a href="https://ml.berkeley.edu/blog/posts/vq-vae/" target="_blank" rel="noopener noreffer ">this blog</a>.</p>
<p>But in simple terms:</p>
<ul>
<li>a <strong>codebook</strong> (list) of <em>K</em> arrays, each filled with <code>dim</code> random floating point values, is created for the model</li>
<li>the encoder converts a spectrogram to a series of arrays of length <code>dim</code>. for each array produced,
<ul>
<li>the <strong>codebook</strong> array with the closest euclidean distance is picked.</li>
<li>the <strong>index</strong> of each codebook array picked corresponds to the <strong>MEL token</strong></li>
</ul>
</li>
<li>the decoder converts the series of codebook arrays back into a spectrogram; the whole network is trained to minimise the reconstruction loss of that spectrogram</li>
</ul>
<p>The <strong>codebook</strong> here is the problem. A fresh VQVAE, trained from scratch, would have a <strong>completely different codebook</strong>, and would fail to learn the same internal representations for audio.</p>
<p>Or, to be more concrete: if, given a short audio clip, the <strong>original VQVAE</strong> produces the MEL tokens <code>[39, 3258, 1892, 8101]</code>, the <strong>new VQVAE</strong> is extremely likely to produce a <strong>completely different list</strong> of tokens, like <code>[2814, 3250, 982, 5832]</code>, even if trained perfectly to negligible loss.</p>
<p>So, if you trained a fresh VQVAE from scratch, you would have to train <strong>the entirety of TorToiSe</strong> from scratch (ignoring the vocoder) as well to get it to work. Not ideal.</p>
<p>Surely there has to be some way to recreate the original VQVAE?</p>
<h2 id="proposal">Proposal</h2>
<p>When I quoted @lexman_ai above, he said:</p>
<blockquote>
<p>So you won&rsquo;t be able to do it without re-training <strong>some</strong> of the models needed from scratch.</p>
</blockquote>
<p><em>Some</em> of the models doesn&rsquo;t sound like <em>all</em> of the models. Could it be possible to train a new VQVAE, while avoiding the problems I mentioned above?</p>
<hr>
<p><em>Everything below this line is baseless speculation, so take it with a heap of salt</em></p>
<div class="details admonition note">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw" aria-hidden="true"></i>Baseless Speculation<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>Formally, we can define the encoder/decoder pair of the VQVAE as functions $f(x) = z, g(z) = \hat x$, where</p>
<p>$$g(f(x)) \approx x$$</p>
<p><strong>Informally</strong>, <code>x</code> is a mel spectrogram, <code>z</code> is a list of MEL tokens, and <code>f</code> is a 1D resnet.</p>
<p>Formally, our goal is to create a new encoder, $f&rsquo;(x)$, such that</p>
<p>$$f&rsquo;(x) \approx f(x)$$</p>
<p>for a varied distribution of values of $x$ and $z$.</p>
<p><strong>Informally</strong>, you <em>could</em> generate a large dataset of synthetic <code>[x,z]</code> pairs by running inference on TorToiSe&rsquo;s GPT model a lot of times, and train an encoder directly to try to predict <code>z</code> from <code>x</code>. You would have to replace the overall reconstruction loss function of the VQVAE with a different loss function (MSE?) that minimises the difference between the expected cookbook array vs the output array of the encoder, and also change up all of the training hyperparameters.</p>
<p>Would it work? I don&rsquo;t know, but I can imagine a number of ways this could go wrong:</p>
<ul>
<li>The synthetic <code>[x,z]</code> pairs are simply too low quality for a model to converge,</li>
<li>The specific random cookbook arrays used in the original VQVAE are important in some way that prevents a resnet with a different cookbook from learning to predict the right tokens</li>
<li>i missed something obvious (it happens)</li>
</ul>
<p>If I was really going to try this idea, I might do something like:</p>
<ul>
<li>using random lines + random conditioning latents from a large tts dataset, generate (with the GPT model) a low-quality unfiltered large dataset, plus a higher quality smaller dataset filtered with CLVP</li>
<li>pre-train an encoder &amp; decoder (separately) on the large dataset, before fine-tuning on the smaller dataset</li>
<li>switch back to the original VQVAE training method with reconstruction loss, and fine-tune the entire VQVAE on librilight or some other dataset</li>
<li>fine-tune the rest of tortoise to account for the differences between this reconstructed model &amp;&amp; the original VQVAE</li>
</ul>
</div>
        </div>
    </div>
<p>But it might not work. And if it doesn&rsquo;t work, and it is truly impossible to fine-tune tortoise without training from scratch, then I can only congratulate the creator of TorToiSe for his ingenuity.</p></div></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.120.1">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://shentongguangda.github.io" target="_blank">第一桶鲸</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/mhchem.min.js"></script><script type="text/javascript" src="/lib/mermaid/mermaid.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"id-1":"graph LR;\n    Audio[Training Spectrogram] --\u003e|ConditioningEncoder| ConditioningEncoding\n    Audio[Training Spectrogram] --\u003e|VQVAE| MELTokens\n    Text[Training Text] --\u003e|VocabBpeTokenizer| TextTokens\n    subgraph A [Training]\n    ConditioningEncoding --\u003e GPT\n    TextTokens --\u003e GPT\n    MELTokens --\u003e GPT\n    MELTokens --\u003e CLVP\n    TextTokens --\u003e CLVP\n    end","id-2":"graph LR;\n    Audio[Input reference audio] --\u003e|ConditioningEncoder| ConditioningEncoding\n    Text[Input Text] --\u003e|VocabBpeTokenizer| TextTokens\n    ConditioningEncoding --\u003e GPT\n    TextTokens --\u003e GPT\n    subgraph autoregression\n    GPT --\u003e MELTokens\n    end\n    subgraph A [top-k ranking]\n    MELTokens --\u003e CLVP\n    TextTokens --\u003e CLVP\n    end"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
